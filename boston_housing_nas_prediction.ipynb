{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boston Housing Price Prediction with Neural Architecture Search (NAS)\n",
    "\n",
    "This notebook implements Neural Architecture Search (NAS) to find the optimal neural network architecture for predicting Boston housing prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('boston_house_prices.csv', skiprows=1)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('MEDV', axis=1)\n",
    "y = df['MEDV']\n",
    "\n",
    "# Split data: 70% train, 15% validation, 15% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)  # 0.176 of 0.85 ≈ 0.15 of total\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data standardization completed.\")\n",
    "print(f\"Feature mean (should be ~0): {X_train_scaled.mean(axis=0).mean():.6f}\")\n",
    "print(f\"Feature std (should be ~1): {X_train_scaled.std(axis=0).mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Neural Architecture Search Space\n",
    "\n",
    "We'll implement a simple NAS approach that searches over:\n",
    "- Number of hidden layers (1-4)\n",
    "- Number of neurons per layer (16, 32, 64, 128)\n",
    "- Activation functions (relu, tanh, elu)\n",
    "- Dropout rates (0.0, 0.1, 0.2, 0.3)\n",
    "- Learning rates (0.001, 0.01, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search space\n",
    "search_space = {\n",
    "    'num_layers': [1, 2, 3, 4],\n",
    "    'neurons': [16, 32, 64, 128],\n",
    "    'activation': ['relu', 'tanh', 'elu'],\n",
    "    'dropout': [0.0, 0.1, 0.2, 0.3],\n",
    "    'learning_rate': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "print(\"Neural Architecture Search Space:\")\n",
    "for param, values in search_space.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "total_combinations = (len(search_space['num_layers']) * \n",
    "                     len(search_space['neurons']) * \n",
    "                     len(search_space['activation']) * \n",
    "                     len(search_space['dropout']) * \n",
    "                     len(search_space['learning_rate']))\n",
    "print(f\"\\nTotal possible architectures: {total_combinations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Model Creation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_layers, neurons, activation, dropout, learning_rate, input_dim):\n",
    "    \"\"\"\n",
    "    Create a neural network model with specified architecture.\n",
    "    \n",
    "    Parameters:\n",
    "    - num_layers: Number of hidden layers\n",
    "    - neurons: Number of neurons per layer\n",
    "    - activation: Activation function\n",
    "    - dropout: Dropout rate\n",
    "    - learning_rate: Learning rate for optimizer\n",
    "    - input_dim: Number of input features\n",
    "    \n",
    "    Returns:\n",
    "    - Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(layers.Input(shape=(input_dim,)))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for i in range(num_layers):\n",
    "        model.add(layers.Dense(neurons, activation=activation))\n",
    "        if dropout > 0:\n",
    "            model.add(layers.Dropout(dropout))\n",
    "    \n",
    "    # Output layer (regression - single neuron)\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement Random Search NAS\n",
    "\n",
    "We'll use random search to sample architectures from the search space and evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search_nas(n_iterations=30, epochs=100, patience=15, verbose=0):\n",
    "    \"\"\"\n",
    "    Perform random search over neural architecture space.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_iterations: Number of architectures to try\n",
    "    - epochs: Maximum training epochs per architecture\n",
    "    - patience: Early stopping patience\n",
    "    - verbose: Training verbosity\n",
    "    \n",
    "    Returns:\n",
    "    - results: List of dictionaries containing architecture configs and scores\n",
    "    - best_config: Best architecture configuration\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_config = None\n",
    "    \n",
    "    input_dim = X_train_scaled.shape[1]\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "    \n",
    "    print(f\"Starting Random Search NAS with {n_iterations} iterations...\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Sample random architecture\n",
    "        config = {\n",
    "            'num_layers': np.random.choice(search_space['num_layers']),\n",
    "            'neurons': np.random.choice(search_space['neurons']),\n",
    "            'activation': np.random.choice(search_space['activation']),\n",
    "            'dropout': np.random.choice(search_space['dropout']),\n",
    "            'learning_rate': np.random.choice(search_space['learning_rate'])\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nIteration {i+1}/{n_iterations}\")\n",
    "        print(f\"Config: layers={config['num_layers']}, neurons={config['neurons']}, \"\n",
    "              f\"activation={config['activation']}, dropout={config['dropout']}, \"\n",
    "              f\"lr={config['learning_rate']}\")\n",
    "        \n",
    "        # Create and train model\n",
    "        model = create_model(\n",
    "            num_layers=config['num_layers'],\n",
    "            neurons=config['neurons'],\n",
    "            activation=config['activation'],\n",
    "            dropout=config['dropout'],\n",
    "            learning_rate=config['learning_rate'],\n",
    "            input_dim=input_dim\n",
    "        )\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            validation_data=(X_val_scaled, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss = min(history.history['val_loss'])\n",
    "        val_mae = min(history.history['val_mae'])\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'config': config.copy(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_mae': val_mae,\n",
    "            'epochs_trained': len(history.history['loss'])\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}, Epochs: {result['epochs_trained']}\")\n",
    "        \n",
    "        # Update best config\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_config = config.copy()\n",
    "            print(f\"*** New best architecture found! Val Loss: {val_loss:.4f} ***\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Random Search NAS completed!\")\n",
    "    \n",
    "    return results, best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NAS (using fewer iterations for faster execution)\n",
    "# Increase n_iterations for more thorough search\n",
    "results, best_config = random_search_nas(n_iterations=30, epochs=100, patience=15, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze NAS Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame([\n",
    "    {**r['config'], 'val_loss': r['val_loss'], 'val_mae': r['val_mae'], 'epochs': r['epochs_trained']}\n",
    "    for r in results\n",
    "])\n",
    "\n",
    "# Sort by validation loss\n",
    "results_df = results_df.sort_values('val_loss')\n",
    "\n",
    "print(\"\\nTop 10 Architectures:\")\n",
    "print(results_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize NAS results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Val Loss by number of layers\n",
    "axes[0, 0].boxplot([results_df[results_df['num_layers']==i]['val_loss'].values \n",
    "                     for i in sorted(results_df['num_layers'].unique())])\n",
    "axes[0, 0].set_xticklabels(sorted(results_df['num_layers'].unique()))\n",
    "axes[0, 0].set_xlabel('Number of Layers')\n",
    "axes[0, 0].set_ylabel('Validation Loss')\n",
    "axes[0, 0].set_title('Val Loss by Number of Layers')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Val Loss by neurons\n",
    "axes[0, 1].boxplot([results_df[results_df['neurons']==i]['val_loss'].values \n",
    "                     for i in sorted(results_df['neurons'].unique())])\n",
    "axes[0, 1].set_xticklabels(sorted(results_df['neurons'].unique()))\n",
    "axes[0, 1].set_xlabel('Number of Neurons')\n",
    "axes[0, 1].set_ylabel('Validation Loss')\n",
    "axes[0, 1].set_title('Val Loss by Number of Neurons')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Val Loss by activation\n",
    "axes[0, 2].boxplot([results_df[results_df['activation']==i]['val_loss'].values \n",
    "                     for i in sorted(results_df['activation'].unique())])\n",
    "axes[0, 2].set_xticklabels(sorted(results_df['activation'].unique()))\n",
    "axes[0, 2].set_xlabel('Activation Function')\n",
    "axes[0, 2].set_ylabel('Validation Loss')\n",
    "axes[0, 2].set_title('Val Loss by Activation Function')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Val Loss by dropout\n",
    "axes[1, 0].boxplot([results_df[results_df['dropout']==i]['val_loss'].values \n",
    "                     for i in sorted(results_df['dropout'].unique())])\n",
    "axes[1, 0].set_xticklabels(sorted(results_df['dropout'].unique()))\n",
    "axes[1, 0].set_xlabel('Dropout Rate')\n",
    "axes[1, 0].set_ylabel('Validation Loss')\n",
    "axes[1, 0].set_title('Val Loss by Dropout Rate')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Val Loss by learning rate\n",
    "axes[1, 1].boxplot([results_df[results_df['learning_rate']==i]['val_loss'].values \n",
    "                     for i in sorted(results_df['learning_rate'].unique())])\n",
    "axes[1, 1].set_xticklabels(sorted(results_df['learning_rate'].unique()))\n",
    "axes[1, 1].set_xlabel('Learning Rate')\n",
    "axes[1, 1].set_ylabel('Validation Loss')\n",
    "axes[1, 1].set_title('Val Loss by Learning Rate')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Overall distribution\n",
    "axes[1, 2].hist(results_df['val_loss'], bins=20, edgecolor='black', color='skyblue')\n",
    "axes[1, 2].axvline(results_df['val_loss'].min(), color='r', linestyle='--', label='Best')\n",
    "axes[1, 2].set_xlabel('Validation Loss')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "axes[1, 2].set_title('Distribution of Validation Loss')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Final Model with Best Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Architecture Found:\")\n",
    "print(\"=\"*50)\n",
    "for param, value in best_config.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train final model with best configuration\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "best_model = create_model(\n",
    "    num_layers=best_config['num_layers'],\n",
    "    neurons=best_config['neurons'],\n",
    "    activation=best_config['activation'],\n",
    "    dropout=best_config['dropout'],\n",
    "    learning_rate=best_config['learning_rate'],\n",
    "    input_dim=input_dim\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with more epochs for final model\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-6)\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
    "axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('Training and Validation MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = best_model.predict(X_train_scaled).flatten()\n",
    "y_val_pred = best_model.predict(X_val_scaled).flatten()\n",
    "y_test_pred = best_model.predict(X_test_scaled).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "val_rmse = np.sqrt(val_mse)\n",
    "val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*70)\n",
    "print(\"NEURAL ARCHITECTURE SEARCH - FINAL MODEL PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTraining Set:\")\n",
    "print(f\"  R² Score: {train_r2:.4f}\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  MAE: {train_mae:.4f}\")\n",
    "print(f\"  MSE: {train_mse:.4f}\")\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "print(f\"  R² Score: {val_r2:.4f}\")\n",
    "print(f\"  RMSE: {val_rmse:.4f}\")\n",
    "print(f\"  MAE: {val_mae:.4f}\")\n",
    "print(f\"  MSE: {val_mse:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "print(f\"  R² Score: {test_r2:.4f}\")\n",
    "print(f\"  RMSE: {test_rmse:.4f}\")\n",
    "print(f\"  MAE: {test_mae:.4f}\")\n",
    "print(f\"  MSE: {test_mse:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction vs Actual plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train, y_train_pred, alpha=0.5, color='blue')\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Price')\n",
    "axes[0].set_ylabel('Predicted Price')\n",
    "axes[0].set_title(f'Training Set\\nR² = {train_r2:.4f}, RMSE = {train_rmse:.4f}')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation set\n",
    "axes[1].scatter(y_val, y_val_pred, alpha=0.5, color='green')\n",
    "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual Price')\n",
    "axes[1].set_ylabel('Predicted Price')\n",
    "axes[1].set_title(f'Validation Set\\nR² = {val_r2:.4f}, RMSE = {val_rmse:.4f}')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Test set\n",
    "axes[2].scatter(y_test, y_test_pred, alpha=0.5, color='red')\n",
    "axes[2].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[2].set_xlabel('Actual Price')\n",
    "axes[2].set_ylabel('Predicted Price')\n",
    "axes[2].set_title(f'Test Set\\nR² = {test_r2:.4f}, RMSE = {test_rmse:.4f}')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "test_residuals = y_test - y_test_pred\n",
    "\n",
    "# Residual plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Residual plot\n",
    "axes[0].scatter(y_test_pred, test_residuals, alpha=0.5, color='purple')\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Predicted Price')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "axes[0].set_title('Residual Plot (Test Set)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual distribution\n",
    "axes[1].hist(test_residuals, bins=30, edgecolor='black', color='lightcoral')\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Residuals')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Residual Distribution (Test Set)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"NEURAL ARCHITECTURE SEARCH - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. Search Configuration:\")\n",
    "print(f\"   - Total architectures evaluated: {len(results)}\")\n",
    "print(f\"   - Search space size: {total_combinations}\")\n",
    "print(f\"   - Search coverage: {len(results)/total_combinations*100:.2f}%\")\n",
    "\n",
    "print(\"\\n2. Best Architecture:\")\n",
    "for param, value in best_config.items():\n",
    "    print(f\"   - {param}: {value}\")\n",
    "\n",
    "print(\"\\n3. Performance Comparison:\")\n",
    "print(f\"   - Best Val Loss from NAS: {results_df['val_loss'].min():.4f}\")\n",
    "print(f\"   - Worst Val Loss from NAS: {results_df['val_loss'].max():.4f}\")\n",
    "print(f\"   - Average Val Loss: {results_df['val_loss'].mean():.4f}\")\n",
    "print(f\"   - Improvement (best vs avg): {(results_df['val_loss'].mean() - results_df['val_loss'].min())/results_df['val_loss'].mean()*100:.2f}%\")\n",
    "\n",
    "print(\"\\n4. Final Model Test Performance:\")\n",
    "print(f\"   - R² Score: {test_r2:.4f} ({test_r2*100:.2f}% variance explained)\")\n",
    "print(f\"   - RMSE: ${test_rmse:.2f}k\")\n",
    "print(f\"   - MAE: ${test_mae:.2f}k\")\n",
    "\n",
    "print(\"\\n5. Key Insights:\")\n",
    "print(f\"   - NAS found an architecture that outperforms random configurations\")\n",
    "print(f\"   - The model generalizes well (Train R²={train_r2:.4f}, Test R²={test_r2:.4f})\")\n",
    "print(f\"   - Average prediction error: ${test_mae:.2f}k\")\n",
    "\n",
    "print(\"\\n6. Top 3 Hyperparameter Insights:\")\n",
    "for param in ['num_layers', 'neurons', 'activation']:\n",
    "    best_value = results_df.groupby(param)['val_loss'].mean().idxmin()\n",
    "    print(f\"   - Best {param}: {best_value}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
